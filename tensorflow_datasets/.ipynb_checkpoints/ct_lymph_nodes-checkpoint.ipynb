{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CT_Lymph_Nodes dataset.\"\"\"\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow_datasets.core import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import pydicom\n",
    "\n",
    "# BibTeX citation\n",
    "_CITATION = \"\"\"\n",
    "\\@misc{CT_Lymph_Nodes_Citation,\n",
    "  doi = {10.1007/978-3-319-10404-1_65},\n",
    "  url = {https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes#12d41e510fe547b59000cd90afb8dbf2},\n",
    "  author = {Roth, Holger R., Lu, Le, Seff, Ari, Cherry, Kevin M., Hoffman, Joanne, Wang, Shijun, Liu, Jiamin, Turkbey, Evrim and Summers, Ronald M.},\n",
    "  title = {A New 2.5D Representation for Lymph Node Detection Using Random Sets of Deep Convolutional Neural Network Observations},\n",
    "  publisher = {Springer International Publishing},\n",
    "  year = {2014},\n",
    "}\n",
    "@article{TCIA_Citation,\n",
    "  author = {\n",
    "    K. Clark and B. Vendt and K. Smith and J. Freymann and J. Kirby and\n",
    "    P. Koppel and S. Moore and S. Phillips and D. Maffitt and M. Pringle and\n",
    "    L. Tarbox and F. Prior\n",
    "  },\n",
    "  title = {{The Cancer Imaging Archive (TCIA): Maintaining and Operating a\n",
    "  Public Information Repository}},\n",
    "  journal = {Journal of Digital Imaging},\n",
    "  volume = {26},\n",
    "  month = {December},\n",
    "  year = {2013},\n",
    "  pages = {1045-1057},\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Data Description\n",
    "_DESCRIPTION = \"\"\"\n",
    "This dataset contains 110,013 Computed Tomography (CT) images of the mediastinum \n",
    "and abdomen in which lymph node positions are marked by radiologists at the \n",
    "National Institutes of Health, Clinical Center. These 10,013 images consist of \n",
    "388 mediastinal lymph nodes that come from 90 patients and a total of 595 \n",
    "abdominal lymph nodes in 86 patients. All images are of 512*512 pixel arrays. \n",
    "\"\"\"\n",
    "\n",
    "#TODO DOWNLOAD LINKS\n",
    "\n",
    "class CT_Lymph_Nodes(tfds.core.GeneratorBasedBuilder):\n",
    "  \"\"\"This is a dataset containing CT scans of lymph nodes from NIH\"\"\"\n",
    "\n",
    "  #Set up version.\n",
    "  VERSION = tfds.core.Version('1.0.0')\n",
    "\n",
    "  MANUAL_DOWNLOAD_INSTRUCTIONS = \"\"\"\\\n",
    "  You can download the images from\n",
    "  https://console.cloud.google.com/storage/browser/bme590/jingjing\n",
    "  Please put all files in manual_dir.\n",
    "  \"\"\"\n",
    "  \n",
    "\n",
    "  def _info(self):\n",
    "    # TODO(my_dataset): Specifies the tfds.core.DatasetInfo object\n",
    "    return tfds.core.DatasetInfo(\n",
    "        builder=self,\n",
    "        # This is the description that will appear on the datasets page.\n",
    "        description=_DESCRIPTION,\n",
    "        # tfds.features.FeatureConnectors\n",
    "        features=tfds.features.FeaturesDict({\n",
    "            \n",
    "            # These are the features of your dataset like images, labels ...\n",
    "     \n",
    "        # If there's a common (input, target) tuple from the features,\n",
    "        # specify them here. They'll be used if as_supervised=True in\n",
    "        # builder.as_dataset.\n",
    "        \n",
    "        #The CT image\n",
    "        'image' : tfds.features.Tensor(shape=(512,512),dtype=tf.int16),\n",
    "        ## The mask\n",
    "        'mask' : tfds.features.Tensor(shape=(512,512),dtype = tf.int16)\n",
    "        }),\n",
    "        supervised_keys=('image','mask'),\n",
    "        # Homepage of the dataset for documentation\n",
    "        homepage='https://dataset-homepage/',\n",
    "        citation=_CITATION,\n",
    "    )\n",
    "\n",
    "  def _split_generators(self, dl_manager):\n",
    "    \"\"\"Returns SplitGenerators.\"\"\"\n",
    "    \n",
    "    filepath = dl_manager.manual_dir\n",
    "    # There is no predefined tran/val/test split for this dataset\n",
    "    return [\n",
    "        tfds.core.SplitGenerator(\n",
    "            name=tfds.Split.TRAIN,\n",
    "            # These kwargs will be passed to _generate_examples\n",
    "            gen_kwargs={'filepath':filepath},\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "  def _generate_examples(self,filepath):\n",
    "    \"\"\"Yields examples for the CT lymph nodes dataset\n",
    "    Args:\n",
    "        filepath: path to the CT lymph nodes files\n",
    "    Yields:\n",
    "        Dictionariew with images and masks\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    yield 'key', {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_Lymph_Nodes()._info   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset ct__lymph__nodes/1.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/jupyter/tensorflow_datasets/ct__lymph__nodes/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<class '__main__.CT_Lymph_Nodes'> is a built-in class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-188d6e1f00c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CT_Lymph_Nodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/datasets/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datasets/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    362\u001b[0m     dl_manager = self._make_download_manager(\n\u001b[1;32m    363\u001b[0m         \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         download_config=download_config)\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;31m# Create a tmp dir and rename to self._data_dir on successful exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datasets/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_make_download_manager\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mextract_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mmanual_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanual_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0mchecksums_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_checksums_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m         \u001b[0mmanual_dir_instructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdedent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMANUAL_DOWNLOAD_INSTRUCTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFORCE_REDOWNLOAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datasets/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_get_checksums_path\u001b[0;34m(builder)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_checksums_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m   \u001b[0;34m\"\"\"Returns the checksums path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m   \u001b[0mchecksums_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"checksums.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mchecksums_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0mchecksums_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchecksums_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datasets/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datasets/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mcode_path\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcode_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34m\"\"\"Returns the path to the file where the Dataset class is located.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is a built-in class'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <class '__main__.CT_Lymph_Nodes'> is a built-in class"
     ]
    }
   ],
   "source": [
    "tfds.load('CT_Lymph_Nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
